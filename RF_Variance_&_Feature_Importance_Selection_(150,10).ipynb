{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de309727",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "%matplotlib inline\n",
    "\n",
    "#Defining Dictionaries\n",
    "List_features = {}\n",
    "List_feature_importances = {}\n",
    "List_dead_features = ['NAME']\n",
    "\n",
    "for feat_num in range(30,46,1):\n",
    "    \n",
    "    List_dead_features = ['NAME']\n",
    "    iter_counter=0\n",
    "    dead_feature_counter = 6 #arbitrary number higher than 0\n",
    "    \n",
    "    while dead_feature_counter>0:\n",
    "\n",
    "        print(f'Number of features: {feat_num}, Iteration: {iter_counter}')\n",
    "        \n",
    "        #Reading and processing of Features\n",
    "        data = pd.read_csv(\"Mordred_Features.csv\",';',low_memory=False,usecols=lambda x: x not in List_dead_features)\n",
    "        features = pd.get_dummies(data)\n",
    "        labels = np.array(np.log10(features['K_off']))\n",
    "        features = features.drop('K_off', axis = 1)\n",
    "        feature_list = list(features.columns)\n",
    "        features = np.array(features)\n",
    "\n",
    "        #**************************************************************************************************\n",
    "\n",
    "        #Sorting Features by descending Variance\n",
    "        Feat_var = {}\n",
    "        for x in range(len(features[0,:])):\n",
    "            List_scal = []                 #scale all features between 0 and 1\n",
    "            sort = sorted(features[:,x])\n",
    "            mini = sort[0]\n",
    "            maxi = sort[-1]\n",
    "            for i in features[:,x]:\n",
    "                i_scal = (i-mini)/(maxi-mini)\n",
    "                List_scal.append(i_scal)\n",
    "            mean = sum(List_scal)/len(List_scal)\n",
    "\n",
    "            List_var = []\n",
    "            for i in List_scal:\n",
    "                var = ((i-mean)**2)/(len(List_scal)) #calculate variance of features\n",
    "                List_var.append(var)\n",
    "                \n",
    "            variance = sum(List_var)\n",
    "            List_features_dummy = []\n",
    "            \n",
    "            for j in features[:,x]:\n",
    "                List_features_dummy.append(j)\n",
    "            Feat_var[feature_list[x],variance]=List_features_dummy\n",
    "            \n",
    "        Feat_var2 = sorted(Feat_var, key=lambda x: x[1], reverse=True) #sort features by variance\n",
    "        feature_list = []\n",
    "        features = []\n",
    "        \n",
    "        for x in Feat_var2:\n",
    "            feature_list.append(x[0])\n",
    "            features.append(Feat_var[x])\n",
    "        features = np.array(features).transpose()\n",
    "        \n",
    "        #***************************************************************************************************\n",
    "        \n",
    "        #Top feat_num features\n",
    "        features = features[:,0:feat_num]\n",
    "        feature_list = feature_list[0:feat_num]\n",
    "        \n",
    "        #***************************************************************************************************\n",
    "\n",
    "        #Splitting of Data\n",
    "        train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "            features, labels, test_size = 0.20, random_state = 7)\n",
    "\n",
    "        \n",
    "        #Random Forest Regressor\n",
    "        rf = RandomForestRegressor(\n",
    "            n_estimators = 150, random_state = 7, max_depth = 10, max_features='log2') \n",
    "        rf.fit(train_features, train_labels)\n",
    "        predictions_test = rf.predict(test_features)\n",
    "        predictions_train = rf.predict(train_features)\n",
    "        \n",
    "        \n",
    "        #Calculation of Metrics\n",
    "        mae_train = metrics.mean_absolute_error(train_labels, predictions_train)\n",
    "        mse_train = metrics.mean_squared_error(train_labels, predictions_train)\n",
    "        rmse_train = mse_train**0.5\n",
    "        r2_train = r2_score(train_labels, predictions_train)\n",
    "        \n",
    "        mae_test = metrics.mean_absolute_error(test_labels, predictions_test)\n",
    "        mse_test = metrics.mean_squared_error(test_labels, predictions_test)\n",
    "        rmse_test = mse_test**0.5\n",
    "        r2_test = r2_score(test_labels, predictions_test)\n",
    "        \n",
    "        print('MAE (Training Set): ',mae_train)\n",
    "        print('RMSE (Training Set): ',rmse_train)\n",
    "        print('r2 score (Training Set): ', r2_train)\n",
    "        print('')\n",
    "        print('MAE (Test Set): ',mae_test)\n",
    "        print('RMSE (Test Set): ',rmse_test)\n",
    "        print('r2 score (Test Set): ', r2_test)\n",
    "\n",
    "\n",
    "        #Calculation of Cross Validation Scores\n",
    "        n_scores = cross_val_score(rf, train_features, train_labels, cv=7, scoring='r2')\n",
    "        n_score = n_scores.mean()\n",
    "        \n",
    "        print('Cross Validation Values: ',n_scores)\n",
    "        print('Cross Validation Score: ',n_score)\n",
    "\n",
    "        #Calculation of Feature Importances\n",
    "        result = permutation_importance(rf, test_features, test_labels, n_repeats=30, random_state=0)\n",
    "        forest_importances = pd.Series(result.importances_mean, index=feature_list)\n",
    "\n",
    "        #Deleting Features with negative (or below permutation_threshold) importances\n",
    "        List_2 = []\n",
    "        counter3=0\n",
    "        permutation_threshold=0.0 #Can be defined differently\n",
    "        for x in range(0,len(feature_list)):\n",
    "            if forest_importances[x]<permutation_threshold:\n",
    "                List_dead_features.append(feature_list[x])\n",
    "                counter3 = counter3 + 1\n",
    "        dead_feature_counter = counter3\n",
    "\n",
    "        #Add features to dictionary\n",
    "        List_features[feat_num,iter_counter,r2_test,r2_train,n_score]=feature_list\n",
    "        List_feature_importances[feat_num,iter_counter,r2_test,r2_train,n_score]=forest_importances\n",
    "\n",
    "        print('')\n",
    "        print('Used features:', feature_list)\n",
    "        print('Amount of dropped features:', dead_feature_counter)\n",
    "        print('')\n",
    "        print('****************************************************************')\n",
    "        print('')\n",
    "        \n",
    "        iter_counter = iter_counter+1\n",
    "\n",
    "print('All models sorted by descending Cross Validation Scores:')    \n",
    "sorted(List_features, key=lambda x:x[4],reverse=True)\n",
    "#print(List_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:my-rdkit-env] *",
   "language": "python",
   "name": "conda-env-my-rdkit-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
